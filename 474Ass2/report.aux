\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data Preprocessing}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Model Training and Hyperparameter Tuning}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Linear Support Vector Machine (SVM)}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Gaussian Support Vector Machine (SVM)}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Neural Network}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Linear SVM Performance}{2}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Cross-validation accuracy as a function of the regularization parameter $C$ for a Linear SVM. The plot illustrates how varying $C$ affects the model's ability to generalize, with higher values of $C$ reducing regularization and potentially leading to overfitting. The optimal value of $C$ balances bias and variance to maximize cross-validation accuracy.}}{2}{}\protected@file@percent }
\newlabel{fig:svm_c}{{1}{2}{}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training and test error as a function of the regularization parameter $C$ for a Linear SVM. The plot highlights the trade-off between training error and test error as $C$ varies. Lower values of $C$ increase regularization, potentially underfitting the data, while higher values may lead to overfitting, as indicated by the divergence between training and test errors.}}{3}{}\protected@file@percent }
\newlabel{fig:svm_error}{{2}{3}{}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Gaussian SVM Performance}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Cross-validation accuracy as a function of the kernel coefficient $\gamma $ for a Gaussian SVM. The plot demonstrates how $\gamma $ impacts model performance, with an optimal value balancing the trade-off between underfitting and overfitting. Extremely high values of $\gamma $ may lead to overfitting, while very low values may result in underfitting.}}{4}{}\protected@file@percent }
\newlabel{fig:gaussian_gamma}{{3}{4}{}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Neural Network Performance}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Cross-validation accuracy as a function of the number of hidden nodes in a Neural Network. The plot shows how increasing the number of hidden nodes affects the model's capacity to learn complex patterns. However, too many nodes may lead to overfitting, while too few may result in underfitting.}}{5}{}\protected@file@percent }
\newlabel{fig:nn_nodes}{{4}{5}{}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Training and test error as a function of the number of epochs in a Neural Network. The plot highlights the relationship between training duration and model performance. Early stopping may be necessary to prevent overfitting, as indicated by the divergence between training and test errors at higher epochs.}}{5}{}\protected@file@percent }
\newlabel{fig:nn_epochs}{{5}{5}{}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Final Model Comparison}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of test errors for Linear SVM, Gaussian SVM, and Neural Network. The plot summarizes the generalization performance of each model, highlighting their relative strengths and weaknesses. The Gaussian SVM demonstrates improved generalization over the Linear SVM, while the Neural Network achieves competitive performance, suggesting its suitability for complex datasets.}}{6}{}\protected@file@percent }
\newlabel{fig:final_comparison}{{6}{6}{}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{6}{}\protected@file@percent }
\gdef \@abspage@last{6}
